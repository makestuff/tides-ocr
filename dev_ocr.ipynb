{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1f62706",
   "metadata": {},
   "source": [
    "# Play with Jupyter and OpenCV\n",
    "Load an entire page, crop to get only the times & depths, detect all the symbols, and extract the columns of symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tide_ocr\n",
    "from matplotlib import pyplot as plt\n",
    "cropped = tide_ocr.image_load_crop_fixup(\"2024/00.png\", 900, 1100, 7200)\n",
    "f = plt.figure(figsize=(20, 20))\n",
    "plt.imshow(tide_ocr.fix_color(cropped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcd5198",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342bb1af-2d04-43cf-b0f5-68ccddffa5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tide_ocr\n",
    "\n",
    "ocr = dict()\n",
    "\n",
    "#for i in range(2016, 2022):\n",
    "#  ocr[i] = tide_ocr.SimpleOCR(str(i), i, 830, 7000, 1100, False, 0, 48)\n",
    "\n",
    "#ocr[2022] = tide_ocr.SimpleOCR(\"2022\", 2022, 900, 7200, 1470, False, 0, 48)\n",
    "#ocr[2023] = tide_ocr.SimpleOCR(\"2023\", 2023, 900, 7200, 1100, False, 0, 48)\n",
    "ocr[2024] = tide_ocr.SimpleOCR(\"2024\", 2024, 900, 7200, 1100, True, 0, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8b7ab",
   "metadata": {},
   "source": [
    "We can use OpenCV to extract bounding-boxes for all the symbols on a page, and using the knowledge that each image forms several columns of digits, we can extract a row of symbols and view them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7938446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the symbols from rows 8, 9, 10 & 11 from the \"London Bridge\" column on page 20\n",
    "ocr[2024].display_column(\"00.png\", True, 0, 4, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979e294",
   "metadata": {},
   "source": [
    "Now we can read all 48 pages, building up a list of unique symbols. We expect to see only 0-9, \".\" and \"-\". This works by maintaining a list of symbols, and xor'ing each new symbol with all the known ones. An xor of two identical symbols will produce an all-black result. An xor of two almost-identical symbols will produce a result with fewer than 30 white pixels. This number 30 is kinda arbitrary. Set it lower and it will find more symbols that it thinks are distinct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f33fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in ocr.keys():\n",
    "  print(y)\n",
    "  ocr[y].do_ocr()\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac8e4a",
   "metadata": {},
   "source": [
    "We can view the list of 64 distinct symbols it found. Clearly they are only 0-9, \".\" and \"-\", but slightly different (e.g a few pixels shaved off here and there)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9500f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr[2024].show_symbols()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbee0b2",
   "metadata": {},
   "source": [
    "Even though the computer thinks there are 64 distinct symbols, there are clearly only 11: digits 0-9, \".\" and \"-\". We can visually identify them for the computer, providing a way to actually identify each symbol programmatically, we can parse all the entries in the tide table to get an array of seven columns, each containing ~1400 rows of (date, isDST, HW/LW, height):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c793af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadata import METADATA\n",
    "from metadata import Location\n",
    "results = dict()\n",
    "#results[2016] = ocr[2016].parse_all(\n",
    "#  \"0345\" + \".896\" +\n",
    "#  \"7120\" + \"3859\" +\n",
    "#  \"62\"\n",
    "#)\n",
    "#results[2017] = ocr[2017].parse_all(\n",
    "#  \"0134\" + \".279\" +\n",
    "#  \"5869\" + \"0538\" +\n",
    "#  \"6208\" + \"3628\" +\n",
    "#  \"0623\" + \"9-\"\n",
    "#)\n",
    "#results[2018] = ocr[2018].parse_all(\n",
    "#  \"0426\" + \".138\" +\n",
    "#  \"5973\" + \"0962\" +\n",
    "#  \"8-\"\n",
    "#)\n",
    "#results[2019] = ocr[2019].parse_all(\n",
    "#  \"016.\" + \"7438\" +\n",
    "#  \"2598\" + \"3059\" +\n",
    "#  \"26-\"\n",
    "#)\n",
    "#results[2020] = ocr[2020].parse_all(\n",
    "#  \"034.\" + \"8927\" +\n",
    "#  \"1565\" + \"3602\" +\n",
    "#  \"89-\"\n",
    "#)\n",
    "#results[2021] = ocr[2021].parse_all(\n",
    "#  \"047.\" + \"1658\" +\n",
    "#  \"3299\" + \"8560\" +\n",
    "#  \"32\"\n",
    "#)\n",
    "#results[2022] = ocr[2022].parse_all(\n",
    "#  \"034.\" + \"8956\" +\n",
    "#  \"1275\" + \"2021\" +\n",
    "#  \"4073\" + \"8806\" +\n",
    "#  \"3600\" + \"6913\" +\n",
    "#  \"5393\" + \"6086\" +\n",
    "#  \"8599\" + \"9162\" +\n",
    "#  \"2962\" + \"8899\" +\n",
    "#  \"-\"\n",
    "#)\n",
    "#results[2023] = ocr[2023].parse_all(METADATA[2023][6])\n",
    "results[2024] = ocr[2024].parse_all(METADATA[2024][Location.LONDON_BRIDGE])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb95f6d",
   "metadata": {},
   "source": [
    "Now we can get a whole year of data for London Bridge, in human-readable format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7c682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2024\n",
    "LOCATION = Location.LONDON_BRIDGE\n",
    "old_date = \"\"\n",
    "old_height = 0.0\n",
    "deltas = []\n",
    "times = []\n",
    "g_stat = [0] * 50\n",
    "old_dt = results[YEAR][LOCATION][0][0]\n",
    "flood = [0] * 14  # 14 time buckets, each 10 mins wide, starting at 5h\n",
    "ebb =   [0] * 14\n",
    "longs = []\n",
    "shorts = []\n",
    "for dt, dst, id, h in results[YEAR][LOCATION]:\n",
    "  g = dt - old_dt\n",
    "  g = int(g.seconds/600)\n",
    "  g_stat[g] += 1\n",
    "  if g != 0:\n",
    "    if id == \"HW\":\n",
    "      flood[g-30] += 1\n",
    "    else:\n",
    "      ebb[g-30] += 1\n",
    "  old_dt = dt\n",
    "  d = dt.strftime(\"%Y-%m-%d:\")\n",
    "  t = dt.strftime(\"%H%M\")\n",
    "  if g == 31:\n",
    "    shorts.append(dt)\n",
    "  if g == 43:\n",
    "    longs.append(dt)\n",
    "  col1 = d if d != old_date else \"           \"\n",
    "  print(\"{} {} {}{}({}m)\".format(col1, id, t, \"*\" if dst else \"\", h))\n",
    "  deltas.append(abs(h-old_height))\n",
    "  times.append(dt)\n",
    "  old_date = d\n",
    "  old_height = h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb4b33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Buckets:\")\n",
    "j = 0\n",
    "for i in g_stat:\n",
    "  if j != 0 and i > 0:\n",
    "    print(f\"  {j}: {i}\")\n",
    "  j += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214bd9a",
   "metadata": {},
   "source": [
    "We can print the days with unusually short, and long tides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74492eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Render a datetime as a string\n",
    "def to_str(dt):\n",
    "  return datetime.datetime.strftime(dt, \"%Y-%m-%dT%H:%MZ\")\n",
    "\n",
    "print(\"Short-duration tides:\")\n",
    "for tide in shorts:\n",
    "  print(f\"  {to_str(tide)}\")\n",
    "print(\"\\nLong-duration tides:\")\n",
    "for tide in longs:\n",
    "  print(f\"  {to_str(tide)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d64829",
   "metadata": {},
   "source": [
    "We can plot how the tide range varies from week to week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053884ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt, dates as pltdates\n",
    "\n",
    "x = []\n",
    "y = []\n",
    "old_ord = times[0].toordinal()\n",
    "sum = 0\n",
    "count = 0\n",
    "for t, d in zip(times[1:], deltas[1:]):\n",
    "  this_ord = t.toordinal()\n",
    "  if this_ord != old_ord:\n",
    "    x.append(datetime.datetime.fromordinal(old_ord))\n",
    "    y.append(sum/count)\n",
    "    sum = 0\n",
    "    count = 0\n",
    "    old_ord = this_ord\n",
    "  sum = sum + d\n",
    "  count = count + 1\n",
    "\n",
    "x.append(datetime.datetime.fromordinal(old_ord))\n",
    "y.append(sum/count)\n",
    "\n",
    "days = pltdates.WeekdayLocator(byweekday=pltdates.TU)\n",
    "fig, ax = plt.subplots(1, figsize=(100, 15))\n",
    "ax.plot(x, y)\n",
    "ax.xaxis.set_major_locator(days)\n",
    "fig.autofmt_xdate()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb048064",
   "metadata": {},
   "source": [
    "And we can find the peaks and troughs (i.e the Springs & Neaps):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7612d9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Series\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "s = Series(y)\n",
    "springs, _ = find_peaks(s)\n",
    "neaps, _ = find_peaks(-s)\n",
    "print(\"Springs:\")\n",
    "for k in springs:\n",
    "  print(f\"  {str(x[k])[:10]}: {y[k]:.3f}\")\n",
    "print(\"\\nNeaps:\")\n",
    "for k in neaps:\n",
    "  print(f\"  {str(x[k])[:10]}: {y[k]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5958e7",
   "metadata": {},
   "source": [
    "And we can see the distribution of durations, for flood and ebb:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e6627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ranges = [\n",
    "  \"5h00-5h09\",\n",
    "  \"5h10-5h19\",\n",
    "  \"5h20-5h29\",\n",
    "  \"5h30-5h39\",\n",
    "  \"5h40-5h49\",\n",
    "  \"5h50-5h59\",\n",
    "  \"6h00-6h09\",\n",
    "  \"6h10-6h19\",\n",
    "  \"6h20-6h29\",\n",
    "  \"6h30-6h39\",\n",
    "  \"6h40-6h49\",\n",
    "  \"6h50-6h59\",\n",
    "  \"7h00-7h09\",\n",
    "  \"7h10-7h19\"\n",
    "]\n",
    "fig, ax = plt.subplots(1, figsize=(20, 10))\n",
    "x_axis = np.arange(len(ranges))\n",
    "ax.bar(x_axis -0.2, flood, width=0.4, label=\"Flood\")\n",
    "ax.bar(x_axis +0.2, ebb, width=0.4, label = \"Ebb\")\n",
    "plt.xticks(x_axis, ranges)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bcbcee",
   "metadata": {},
   "source": [
    "Finally, we can look for instances of unusually-low water at London Bridge, and unusually-high water at North Woolwich:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7eeb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "print(\"Unusually low, at London Bridge:\")\n",
    "for i in results[YEAR][Location.LONDON_BRIDGE]:\n",
    "  if i[3] < 0.1:\n",
    "    print(f\"  {to_str(i[0])}: {i[3]}m\")\n",
    "\n",
    "print(\"\\nUnusually high, at North Woolwich:\")\n",
    "for i in results[YEAR][Location.NORTH_WOOLWICH]:\n",
    "  if i[3] > 7.6:\n",
    "    print(f\"  {to_str(i[0])}: {i[3]}m\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
